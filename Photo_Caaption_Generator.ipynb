{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "16YzhpPspDnoKLKVvyX08PEeQ5ejvBfn1",
      "authorship_tag": "ABX9TyPldb8nu9fLEfY8ftfrM938",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeonjungko/PhotoCaptionGenerator/blob/main/Photo_Caaption_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afLqEZ1ONp8U",
        "outputId": "251fa020-32e2-4a91-b152-42eed7dc42fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scipy: 1.7.3\n",
            "numpy: 1.21.6\n",
            "matplotlib: 3.2.2\n",
            "pandas: 1.3.5\n",
            "statsmodels: 0.12.2\n",
            "sklearn: 1.0.2\n"
          ]
        }
      ],
      "source": [
        "import scipy\n",
        "print('scipy: %s' % scipy.__version__)\n",
        "# numpy\n",
        "import numpy\n",
        "print('numpy: %s' % numpy.__version__)\n",
        "# matplotlib\n",
        "import matplotlib\n",
        "print('matplotlib: %s' % matplotlib.__version__)\n",
        "# pandas\n",
        "import pandas\n",
        "print('pandas: %s' % pandas.__version__)\n",
        "# statsmodels\n",
        "import statsmodels\n",
        "print('statsmodels: %s' % statsmodels.__version__)\n",
        "# scikit-learn\n",
        "import sklearn\n",
        "print('sklearn: %s' % sklearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install theano"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1zznikvN0oT",
        "outputId": "8ac041a5-503a-4009-c236-5612fa1a96a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from theano) (1.7.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from theano) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from theano) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# theano\n",
        "import theano\n",
        "print('theano: %s' % theano.__version__)\n",
        "# tensorflow\n",
        "import tensorflow as tf\n",
        "print('tensorflow: %s' % tf.__version__)\n",
        "# keras\n",
        "import keras\n",
        "print('keras: %s' % keras.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZPok4S-NvDh",
        "outputId": "6c78fbc4-ac09-4cf2-ef40-0b945a62ba90"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theano: 1.0.5\n",
            "tensorflow: 2.9.2\n",
            "keras: 2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "1nqBsdqXN5R8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJ_PATH = '/content/drive/MyDrive/Colab Notebooks/Photo Caption Generator'"
      ],
      "metadata": {
        "id": "FEJohbMFjkPQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip '/content/drive/MyDrive/Colab Notebooks/Photo Caption Generator/Dataset/Flickr8k_Dataset.zip' -d '/content/drive/MyDrive/Colab Notebooks/Photo Caption Generator/Dataset'\n",
        "# !unzip '/content/drive/MyDrive/Colab Notebooks/Photo Caption Generator/Dataset/Flickr8k_text.zip' -d '/content/drive/MyDrive/Colab Notebooks/Photo Caption Generator/Dataset/Flickr8k_text'"
      ],
      "metadata": {
        "id": "HK4CcMtfR6N3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "import pathlib\n",
        "from pickle import dump\n",
        "from keras.applications.vgg16 import VGG16\n",
        "# from keras.preprocessing.image import load_img\n",
        "from keras.utils import load_img\n",
        "from keras.utils import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model"
      ],
      "metadata": {
        "id": "hKS_ik0MTaDI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(directory):\n",
        "\t# load the model\n",
        "\tmodel = VGG16()\n",
        "\t# re-structure the model\n",
        "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "\t# summarize\n",
        "\tprint(model.summary())\n",
        "\t# extract features from each photo\n",
        "\tfeatures = dict()\n",
        "\tfor name in listdir(directory):\n",
        "\t\t# load an image from file\n",
        "\t\tfilename = directory + '/' + name\n",
        "\t\timage = load_img(filename, target_size=(224, 224))\n",
        "\t\t# convert the image pixels to a numpy array\n",
        "\t\timage = img_to_array(image)\n",
        "\t\t# reshape data for the model\n",
        "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t\t# prepare the image for the VGG model\n",
        "\t\timage = preprocess_input(image)\n",
        "\t\t# get features\n",
        "\t\tfeature = model.predict(image, verbose=0)\n",
        "\t\t# get image id\n",
        "\t\timage_id = name.split('.')[0]\n",
        "\t\t# store feature\n",
        "\t\tfeatures[image_id] = feature\n",
        "\t\tprint('>%s' % name)\n",
        "\treturn features"
      ],
      "metadata": {
        "id": "9NFNS7yzUwHU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features from all images\n",
        "# directory = \"/content/drive/MyDrive/Colab Notebooks/Photo Caption Generator/Dataset/Flicker8k_Dataset\"\n",
        "# features = extract_features(directory)\n",
        "# print('Extracted Features: %d' % len(features))\n",
        "# save to file\n",
        "# dump(features, open('/content/drive/MyDrive/Colab Notebooks/Photo Caption Generator/Dataset/features.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "PCJIDdP2WZPf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function to load document\n",
        "def load_doc(filename):\n",
        "  file = open(filename, 'r')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text"
      ],
      "metadata": {
        "id": "YYLTpZRqWhhs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load token text doc\n",
        "token_text_filename = '/content/drive/MyDrive/Colab Notebooks/Photo Caption Generator/Dataset/Flickr8k_text/Flickr8k.token.txt'\n",
        "token_doc = load_doc(token_text_filename)\n",
        "print(token_doc[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAc_nYE9bLi-",
        "outputId": "ebbbfbd3-8e8b-4320-b980-e2cc250a6ccf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function to load descriptions for images\n",
        "def load_descriptions(doc):\n",
        "  mapping = dict()\n",
        "  # process doc line by line\n",
        "  for line in doc.split('\\n'):\n",
        "    # split by whitespace\n",
        "    tokens = line.split()\n",
        "    if len(line) < 2:\n",
        "      continue\n",
        "    # take first token as the image id, the rest as the description\n",
        "    image_id, image_desc = tokens[0], tokens[1:]\n",
        "    # remove .jpg and other unnecessary info from image id\n",
        "    image_id = image_id.split('.')[0]\n",
        "    # convert description tokens back to a single string\n",
        "    image_desc = ' '.join(image_desc)\n",
        "    # create empty list for key if not already created\n",
        "    if image_id not in mapping:\n",
        "      mapping[image_id] = list()\n",
        "    # add description\n",
        "    mapping[image_id].append(image_desc)\n",
        "  return mapping"
      ],
      "metadata": {
        "id": "BwsK58Bcbj21"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse image descriptions\n",
        "descriptions = load_descriptions(token_doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "print(list(descriptions.keys())[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAFph9Uydrfu",
        "outputId": "2e80db93-9bc9-4d07-f382-db3a6edff2e9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: 8092 \n",
            "1000268201_693b08cb0e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The image descriptions are rather clean, so there's not much to do. \n",
        "But let's still \n",
        "1. convert all words to lowercase\n",
        "2. remove all punctuation\n",
        "3. remove all words that are one character or less in length\n",
        "4. remove all words with numbers in them\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MvrfZ1zxe3-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def clean_descriptions(descriptions):\n",
        "  # prepare translation table to remove punctuation\n",
        "  table = str.maketrans('','', string.punctuation)\n",
        "  for _, desc_list in descriptions.items():\n",
        "    for i in range(len(desc_list)):\n",
        "      desc = desc_list[i]\n",
        "      # tokenize\n",
        "      desc = desc.split()\n",
        "      # 1. convert to lowercase\n",
        "      desc = [word.lower() for word in desc]\n",
        "      # 2. remove punctuation from each token\n",
        "      desc = [w.translate(table) for w in desc]\n",
        "      # 3. remove hanging 's' and 'a'\n",
        "      desc = [word for word in desc if len(word)>1]\n",
        "      # 4. remove tokens with numbers in them\n",
        "      desc = [word for word in desc if word.isalpha()]\n",
        "      # store as string\n",
        "      desc_list[i] = ' '.join(desc)"
      ],
      "metadata": {
        "id": "RmduAKBDd2_U"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean image descriptions\n",
        "clean_descriptions(descriptions)\n",
        "print(list(descriptions.keys())[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_znUP2f3gXs6",
        "outputId": "5ed5ed08-6731-477c-ae04-a29fcb3b7aa6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000268201_693b08cb0e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see how the cleaned descriptions fair in terms of the vocab size\n",
        "def to_vocabulary(descriptions):\n",
        "  all_desc = set()\n",
        "  for key in descriptions.keys():\n",
        "    [all_desc.update(d.split()) for d in descriptions[key]]\n",
        "  return all_desc\n",
        "\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foT2sZMYgbRZ",
        "outputId": "e3117ffa-ecbe-40f0-cc97-b815bd432ead"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 8763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change of vocab size from cleaning: 8092 to 8763."
      ],
      "metadata": {
        "id": "tyqKN_WTmeDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function to save dictionary of image identifier and image descriptions to a file\n",
        "def save_descriptions(descriptions, filename):\n",
        "  lines = list()\n",
        "  for key, desc_list in descriptions.items():\n",
        "    for desc in desc_list:\n",
        "      lines.append(key + ' ' + desc)\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename, 'w')\n",
        "  file.write(data)\n",
        "  file.close()\n",
        "\n",
        "# save descriptions to a file\n",
        "save_descriptions(descriptions, f\"{PROJ_PATH}/descriptions.txt\")"
      ],
      "metadata": {
        "id": "GdO5whhZilZY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data"
      ],
      "metadata": {
        "id": "u4cO0Lo-mxVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function to load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "  doc = load_doc(filename)\n",
        "  dataset = list()\n",
        "  for line in doc.split('\\n'):\n",
        "    if len(line) < 1:\n",
        "      continue\n",
        "    # fetch image identifier\n",
        "    identifier = line.split('.')[0]\n",
        "    dataset.append(identifier)\n",
        "  return set(dataset)"
      ],
      "metadata": {
        "id": "lt4jOYpWj-KK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function to load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "  # load document\n",
        "  doc = load_doc(filename)\n",
        "  descriptions = dict()\n",
        "  for line in doc.split('\\n'):\n",
        "    tokens = line.split()\n",
        "    image_id, image_desc = tokens[0], tokens[1:]\n",
        "    if image_id in dataset:\n",
        "      if image_id not in descriptions:\n",
        "        descriptions[image_id] = list()\n",
        "        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "        # store\n",
        "        descriptions[image_id].append(desc)\n",
        "  return descriptions"
      ],
      "metadata": {
        "id": "bzQe66_HpLB3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "\n",
        "# function to load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "  all_features = load(open(filename, 'rb'))\n",
        "  features = {k: all_features[k] for k in dataset}\n",
        "  return features"
      ],
      "metadata": {
        "id": "tx2Bvn9DpyvB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load training dataset (6k)\n",
        "filename = f\"{PROJ_PATH}/Dataset/Flickr8k_text/Flickr_8k.trainImages.txt\"\n",
        "train = load_set(filename)\n",
        "print(f\"Dataset: {len(train)}\")\n",
        "\n",
        "# load training descriptions\n",
        "train_descriptions = load_clean_descriptions(f\"{PROJ_PATH}/descriptions.txt\", train)\n",
        "print(f\"Descriptions: train={len(train_descriptions)}\")\n",
        "\n",
        "# load photo features\n",
        "train_features = load_photo_features(f\"{PROJ_PATH}/Dataset/features.pkl\", train)\n",
        "print(f\"Photos: train={len(train_features)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9jNkVKeq8IP",
        "outputId": "1c905615-531d-4d12-80f0-241332f7ceda"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Photos: train=6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The description text needs to be encoded to numbers before it can be presented ot the model as inputs."
      ],
      "metadata": {
        "id": "_q8iMNqsthAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "pRtxiRRhvIsX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "  all_desc = list()\n",
        "  for key in descriptions.keys():\n",
        "    [all_desc.append(d) for d in descriptions[key]]\n",
        "  return all_desc\n",
        "\n",
        "# function to fit tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "  lines = to_lines(descriptions)\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "6AV4RiJir-Sf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"Vocabulary Size: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2-AFGlGv3is",
        "outputId": "6d8adb47-ba5b-415d-b655-b1d2434d7473"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 3848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
        "  X1, X2, y = list(), list(), list()\n",
        "  # for each image identifier\n",
        "  for key, desc_list in descriptions.items():\n",
        "    for desc in desc_list:\n",
        "      # encode sequence\n",
        "      seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "      # split one sequence into multiple X,y pairs\n",
        "      for i in range(1, len(seq)):\n",
        "        # split into input and output pair\n",
        "        in_seq, out_seq = seq[:i], seq[i]\n",
        "        # pad input sequence\n",
        "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "        # encode output sequence\n",
        "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "        # store\n",
        "        X1.append(photos[key][0])\n",
        "        X2.append(in_seq)\n",
        "        y.append(out_seq)\n",
        "  return array(X1), array(X2), array(y)"
      ],
      "metadata": {
        "id": "IADgyAonwCNi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utlity function to calculate the length of the description with the most words\n",
        "def max_length(descriptions):\n",
        "  lines = to_lines(descriptions)\n",
        "  return max(len(d.split()) for d in lines)"
      ],
      "metadata": {
        "id": "QtSsHqTP2c0Y"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max_length(train_descriptions)\n",
        "print(f\"Description Length:  {max_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO4iHLOT4c69",
        "outputId": "1fc83292-f80c-48b9-9eb7-98497314f378"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Description Length:  30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)"
      ],
      "metadata": {
        "id": "BypcmF-t42F4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input of 4096 elements(image) is processed by VGG-16 to output a 256 element feature vector of the input image.\n",
        "\n",
        "The Word Embedding layer takes in a pre-defined length sequence (34 words) that is fed into an LSTM that outputs a 256 element vector. \n",
        "\n",
        "The final model merges the two 256 element vectors. It then makes a softmax prediction over the entire output vocabulary for the next word in the sequence."
      ],
      "metadata": {
        "id": "3ZaY6jFc3oa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Captioning Model"
      ],
      "metadata": {
        "id": "XcUUr9BL47tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, Add\n",
        "\n",
        "def define_model(vocab_size, max_length):\n",
        "  # feature extractor model\n",
        "  inputs1 = Input(shape=(4096,))\n",
        "  fe1 = Dropout(0.5)(inputs1)\n",
        "  fe2 = Dense(256, activation='relu')(fe1)\n",
        "  \n",
        "  # sequence model\n",
        "  inputs2 = Input(shape=(max_length,))\n",
        "  se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "  se2 = Dropout(0.5)(se1)\n",
        "  se3 = LSTM(256)(se2)\n",
        "\n",
        "  # decoder model\n",
        "  decoder1 = Add()([fe2, se3])\n",
        "  decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "  outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "  # summarize model\n",
        "  print(model.summary())\n",
        "  plot_model(model, to_file=f\"{PROJ_PATH}/model.png\", show_shapes=True)\n",
        "  return model"
      ],
      "metadata": {
        "id": "UV3VehG22tuN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load test set\n",
        "test_set_filename = f\"{PROJ_PATH}/Dataset/Flickr8k_text/Flickr_8k.devImages.txt\"\n",
        "test = load_set(test_set_filename)\n",
        "print(f\"Dataset: {len(test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cQSitV-4Ecl",
        "outputId": "939cada6-14c9-4510-8266-cc4165148ce8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# descriptions\n",
        "test_descriptions = load_clean_descriptions(f\"{PROJ_PATH}/descriptions.txt\", test)\n",
        "print(f\"Descriptions: test={len(test_descriptions)}\")\n",
        "\n",
        "# photo features\n",
        "test_features = load_photo_features(f\"{PROJ_PATH}/Dataset/features.pkl\", test)\n",
        "print(f\"Photos: test={len(test_features)}\")\n",
        "\n",
        "# prepare sequences\n",
        "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caPHP9n552lv",
        "outputId": "3dab1e00-0e2b-4c13-fc96-51d3a23b187c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descriptions: test=1000\n",
            "Photos: test=1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "\n",
        "model = define_model(vocab_size, max_length)"
      ],
      "metadata": {
        "id": "5SOkcBId6UHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c8e0a55-3a4d-4ecb-f232-99aaa5055de9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 4096)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 30, 256)      985088      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 4096)         0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 30, 256)      0           ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 256)          1048832     ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 256)          525312      ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 256)          0           ['dense[0][0]',                  \n",
            "                                                                  'lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 256)          65792       ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 3848)         988936      ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,613,960\n",
            "Trainable params: 3,613,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# define a checkpoint callback\n",
        "checkpoint_file_path = f\"{PROJ_PATH}\" + '/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "checkpoint = ModelCheckpoint(checkpoint_file_path, mointor='val_loss', verbose=1, save_best_only=True, mode='min')"
      ],
      "metadata": {
        "id": "FofLo4wc2Nuk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit model\n",
        "model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TJedKbxh7aC-",
        "outputId": "60a27c33-4ad8-47a7-b999-64a91ca0d14a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 4.45043, saving model to /content/drive/MyDrive/Colab Notebooks/Photo Caption Generator/model-ep001-loss5.095-val_loss4.450.h5\n",
            "1916/1916 - 171s - loss: 5.0949 - val_loss: 4.4504 - 171s/epoch - 89ms/step\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 2: val_loss improved from 4.45043 to 4.15294, saving model to /content/drive/MyDrive/Colab Notebooks/Photo Caption Generator/model-ep002-loss4.245-val_loss4.153.h5\n",
            "1916/1916 - 158s - loss: 4.2455 - val_loss: 4.1529 - 158s/epoch - 83ms/step\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 3: val_loss improved from 4.15294 to 4.05662, saving model to /content/drive/MyDrive/Colab Notebooks/Photo Caption Generator/model-ep003-loss3.897-val_loss4.057.h5\n",
            "1916/1916 - 158s - loss: 3.8973 - val_loss: 4.0566 - 158s/epoch - 82ms/step\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 4: val_loss improved from 4.05662 to 4.05265, saving model to /content/drive/MyDrive/Colab Notebooks/Photo Caption Generator/model-ep004-loss3.656-val_loss4.053.h5\n",
            "1916/1916 - 166s - loss: 3.6555 - val_loss: 4.0527 - 166s/epoch - 87ms/step\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 5: val_loss did not improve from 4.05265\n",
            "1916/1916 - 158s - loss: 3.4716 - val_loss: 4.0942 - 158s/epoch - 82ms/step\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 6: val_loss did not improve from 4.05265\n",
            "1916/1916 - 156s - loss: 3.3149 - val_loss: 4.1536 - 156s/epoch - 82ms/step\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 7: val_loss did not improve from 4.05265\n",
            "1916/1916 - 155s - loss: 3.1809 - val_loss: 4.2801 - 155s/epoch - 81ms/step\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 8: val_loss did not improve from 4.05265\n",
            "1916/1916 - 164s - loss: 3.0601 - val_loss: 4.3760 - 164s/epoch - 85ms/step\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 9: val_loss did not improve from 4.05265\n",
            "1916/1916 - 156s - loss: 2.9561 - val_loss: 4.4881 - 156s/epoch - 81ms/step\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 10: val_loss did not improve from 4.05265\n",
            "1916/1916 - 155s - loss: 2.8618 - val_loss: 4.5773 - 155s/epoch - 81ms/step\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 11: val_loss did not improve from 4.05265\n",
            "1916/1916 - 193s - loss: 2.7697 - val_loss: 4.7253 - 193s/epoch - 101ms/step\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 12: val_loss did not improve from 4.05265\n",
            "1916/1916 - 240s - loss: 2.6899 - val_loss: 4.8582 - 240s/epoch - 125ms/step\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 13: val_loss did not improve from 4.05265\n",
            "1916/1916 - 159s - loss: 2.6172 - val_loss: 5.0706 - 159s/epoch - 83ms/step\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 14: val_loss did not improve from 4.05265\n",
            "1916/1916 - 198s - loss: 2.5451 - val_loss: 5.1432 - 198s/epoch - 103ms/step\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 15: val_loss did not improve from 4.05265\n",
            "1916/1916 - 166s - loss: 2.4782 - val_loss: 5.3323 - 166s/epoch - 87ms/step\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 16: val_loss did not improve from 4.05265\n",
            "1916/1916 - 181s - loss: 2.4212 - val_loss: 5.4622 - 181s/epoch - 95ms/step\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 17: val_loss did not improve from 4.05265\n",
            "1916/1916 - 164s - loss: 2.3605 - val_loss: 5.6349 - 164s/epoch - 86ms/step\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-0c777e2cb302>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX1train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX1test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the Model\n",
        "Generate a description for an example photo using the trained model, then compare the prediction to actual description."
      ],
      "metadata": {
        "id": "lWjpZa9HMugc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import argmax\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "  for word, idx in tokenizer.word_index.items():\n",
        "    if idx == integer:\n",
        "      return word\n",
        "  return None\n",
        "\n",
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "  # seed the generation process\n",
        "  in_text = 'startseq'\n",
        "  # iterate over the whole length of the sequence\n",
        "  for i in range(max_length):\n",
        "    # integer encode input sequence\n",
        "    sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    # pad input\n",
        "    sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "    # predict next word\n",
        "    yhat = model.predict([photo,sequence], verbose=0)\n",
        "    # convert probability to integer\n",
        "    yhat = argmax(yhat)\n",
        "    # map integer to word\n",
        "    word = word_for_id(yhat, tokenizer)\n",
        "    # stop if we cannot map the word\n",
        "    if word == None:\n",
        "      break\n",
        "    # append as input for generating the next word\n",
        "    in_text += ' ' + word\n",
        "    # stop if we predict the end of the sequence\n",
        "    if word == 'endseq':\n",
        "      break\n",
        "  return in_text"
      ],
      "metadata": {
        "id": "5l5MxGCwMwn5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "  actual, predicted = list(), list()\n",
        "  # step over the whole set\n",
        "  for key, desc_list in descriptions.items():\n",
        "    # generate description\n",
        "    yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "    # store actual and predicted\n",
        "    references = [d.split() for d in desc_list]\n",
        "    actual.append(references)\n",
        "    predicted.append(yhat.split())\n",
        "  # calculate BLEU score\n",
        "  print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "  print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "  print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "  print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "metadata": {
        "id": "HVbUQMiiQ7mL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "best_model_so_far = f\"{PROJ_PATH}/model-ep004-loss3.665-val_loss4.049.h5\"\n",
        "model = load_model(best_model_so_far)\n",
        "\n",
        "# evaluate\n",
        "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxbVjO5rRrd2",
        "outputId": "96a5c18e-24df-4874-9f59-404c420990db"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU-1: 0.346666\n",
            "BLEU-2: 0.161968\n",
            "BLEU-3: 0.116074\n",
            "BLEU-4: 0.049976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save tokenizer\n",
        "dump(tokenizer, open(f\"{PROJ_PATH}/tokenizer.pkl\", 'wb'))"
      ],
      "metadata": {
        "id": "q5z5pzDbShYv"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the tokenizer\n",
        "tokenizer = load(open(f\"{PROJ_PATH}/tokenizer.pkl\", 'rb'))\n",
        "max_length = 34"
      ],
      "metadata": {
        "id": "NA7GeR-pS4lG"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features from each photo in the directory\n",
        "def extract_features_single(filename):\n",
        "\t# load the model\n",
        "\tmodel = VGG16()\n",
        "\t# re-structure the model\n",
        "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "\t# load the photo\n",
        "\timage = load_img(filename, target_size=(224, 224))\n",
        "\t# convert the image pixels to a numpy array\n",
        "\timage = img_to_array(image)\n",
        "\t# reshape data for the model\n",
        "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t# prepare the image for the VGG model\n",
        "\timage = preprocess_input(image)\n",
        "\t# get features\n",
        "\tfeature = model.predict(image, verbose=0)\n",
        "\treturn feature\n",
        "\n",
        "example_photo = extract_features_single(f\"{PROJ_PATH}/example.png\")"
      ],
      "metadata": {
        "id": "Oo5iSS7UUYD1"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_descrip = generate_desc(model, tokenizer, example_photo, max_length)\n",
        "print(example_descrip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4B-2XseUrx9",
        "outputId": "23b2ec20-467d-4f8e-91b1-63e11f93f105"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "startseq brown dog is running through the snow endseq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IlIQ7nXjUy6n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}